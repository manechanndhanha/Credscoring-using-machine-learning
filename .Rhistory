# load package FactoMineR and ggplot2
require(ggplot2)
source('C:/Users/dandu/OneDrive/Desktop/310,16-59/CreditScoring-master/Part6_CredScoring_DecisionTrees.R')
source('C:/Users/dandu/OneDrive/Desktop/310,16-59/CreditScoring-master/Part6_CredScoring_DecisionTrees.R')
# load package FactoMineR and ggplot2
require(FactoMineR)
require(ggplot2)
# read cleaned data set
dd = read.csv("CleanCreditScoring.csv", header=TRUE, stringsAsFactors=TRUE)
# select select categorized continuous variables
ddcat = subset(dd, select=c(seniorityR, timeR, ageR, expensesR, incomeR,
assetsR, debtR, amountR, priceR, finratR, savingsR, Status))
# MCA
mca = MCA(ddcat, ncp=40, quali.sup=12, graph=FALSE)
# get the eigenvalues
eigs = mca$eig$eigenvalue
# calculate significant dimension in MCA
nd = sum(eigs>1/length(eigs))
# get factorial coordinates of individuals
Psi = mca$ind$coord
# The first approach is to apply K-means on factorial coordinates
# Let's try k=5 groups
k1 = kmeans(Psi, 5)
# what does k1 contain?
attributes(k1)
# examine the following
k1$size       # size of clusters
k1$withinss   # within cluster variance
k1$centers    # centers coordinates
# between clusters sum of squares
Bss = sum(rowSums(k1$centers^2) * k1$size)
Bss
# within clusters sum of squares
Wss = sum(k1$withinss)
Wss
# total sum of squares
Tss = sum(rowSums(Psi^2))
Tss
Bss + Wss
# let's calculate the decomposition of inertia
Ib1 = 100 * Bss / (Bss + Wss)
Ib1
# let's repeat kmeans, again with k=5
k2 = kmeans(Psi, 5)
# between clusters sum of squares
Bss = sum(rowSums(k2$centers^2) * k2$size)
Wss = sum(k2$withinss)
# total sum of squares
Tss = sum(rowSums(Psi^2))
Bss + Wss    # Tss = Bss + Wss
# let's calculate the decomposition of inertia
Ib2 = 100 * Bss / (Bss + Wss)
Ib2
# let's repeat kmeans, again with k=8
k3 = kmeans(Psi, 8)
# between clusters sum of squares
Bss = sum(rowSums(k3$centers^2) * k3$size)
Wss = sum(k3$withinss)
# total sum of squares
Tss = sum(rowSums(Psi^2))
Bss + Wss    # Tss = Bss + Wss
# let's calculate the decomposition of inertia
Ib3 = 100 * Bss / (Bss + Wss)
Ib3
# Now, let's apply a hierarchical clustering
# first we calculate a distance matrix between individuals
idist = dist(Psi)
# then we apply hclust with method="ward"
# notice the computation cost! (it takes a while to finish)
h1 = hclust(idist, method="ward")
# plot dendrogram
plot(h1, labels=FALSE)
# after checking the dendrogram, how many groups would you choose?
# where would you cut the dendrogram?
# let's try 8 clusters
nc = 8
# let's cut the tree and see the size of clusters
c1 = cutree(h1, nc)
table(c1)
individuals
individuals
individuals
individuals
individuals
